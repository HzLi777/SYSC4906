{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFZIKVNfjDcZ"
   },
   "source": [
    "# Tutorial 11 - Natural Language Processing\n",
    "\n",
    "**Semester:** Fall 2020\n",
    "\n",
    "**Adapted by:** [Kevin Dick](https://kevindick.ai/)\n",
    "\n",
    "**PART I Concepts adapted from:** [Ventsislav Yordanov's](https://medium.com/@ventsislav94) [Introduction to Natural Language Processing for Text](https://towardsdatascience.com/introduction-to-natural-language-processing-for-text-df845750fb63).\n",
    "\n",
    "**PART II Notebooks adapted from:** [HuggingFace's Transformer Notebooks](https://huggingface.co/transformers/notebooks.html) particularly the [Getting Started with Transformers Notebook](https://github.com/huggingface/transformers/blob/master/notebooks/02-transformers.ipynb)\n",
    "\n",
    "**PART III Notebooks adapted from:** [José Eduardo Storopoli](https://github.com/storopoli)'s [Topic Modelling Notebooks](https://github.com/storopoli/topic-modelling/tree/master/Notebooks)\n",
    "\n",
    "---\n",
    "\n",
    "**Tangential Aside**: For anyone planning on pursuing advanced research in the field of NLP, I strongly suggest familiarizing yourselves with [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law) which is a universal distribution of word frequencies within anyy (and all!) languages: [Fantastic (& Quizacious ;) VSauce Video Disccussing the Zipf Mystery](https://youtu.be/fCn8zs912OE)\n",
    "\n",
    "---\n",
    "\n",
    "### PART I: What is Natural Language Processing (NLP)?\n",
    "\n",
    "NLP is a **subfield of machine learning** concerned with the application of **learning algorthms to text and speech**. More generally, NLP-based methods are typically applicable to all sequential-type information (*e.g.* DNA sequences, audio signals, time-series signals, *etc.*) however, they are predominantly used in human language applications.\n",
    "\n",
    "For example, we can use NLP to create systems including:\n",
    "1. **speech recognition** (*e.g.* real-time captioning)\n",
    "2. **document summarization** \n",
    "3. **machine translation**\n",
    "4. **spam detection**\n",
    "5. **named entity recognition**\n",
    "6. **question answering**\n",
    "7. **autocomplete** (*i.e.* predictive typing)\n",
    "\n",
    "**General Data Processing Pipeline:** In order to tackle each of these tasks, we must first process text into a format amenable for use by NLP learning algorithms.\n",
    "\n",
    "### The NLTK Library for the Basics of NLP [(See Details Here)](https://towardsdatascience.com/introduction-to-natural-language-processing-for-text-df845750fb63)\n",
    "NLTK (Natural Language Toolkit) is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to many corpora (large document collections) and lexical resources. Also, it contains a suite of text processing libraries for **classification**, **tokenization**, **stemming**, **tagging**, **parsing**, and **semantic reasoning**. Best of all, NLTK is a free, open source, community-driven project.\n",
    "\n",
    "1. **Sentence Tokenization:** Sentence tokenization (also called **sentence segmentation**) is the problem of *dividing a string of written language into its component sentences*.\n",
    "2. **Word Tokenization**: Word tokenization (also called **word segmentation**) is the problem of *dividing a string of written language into its component words*.\n",
    "3. **Text Lemmatization & Stemming**: The goal of both stemming (crude) and lemmatization (refiined) is to *reduce inflectional forms* and sometimes derivationally related forms of a word to a common base form. For example, \"drive\" & \"drives\" & \"driving\" all have the same semantic meaning and should be combined.\n",
    "4. **Stop Words**: Stop words usually refer to the **most common words** such as “and”, “the”, “a” in a language and when applying machine learning to text, **these words can add a lot of noise** so we remove them.\n",
    "5. **Regex**: A regular expression, regex, or regexp is a sequence of characters that define *a search pattern to apply additional filtering* to our text. For example, we can remove all the non-words characters. In many cases, we don’t need the punctuation marks and it’s easy to remove them with regex.\n",
    "6. **Bag-of-Words**: Machine learning algorithms *cannot work with raw text directly*, we need to convert the text into vectors of numbers (i.e. feature extraction) and the *bag-of-words mode*l is a popular and simple feature extraction technique that *counts the occurrence of each word within a document*.\n",
    "7. **TF-IDF**: One problem with scoring word frequency is that *the most frequent words in the document start to have the highest scores* (frequent words may not havee much “informational gain”) to the model so we penalize words that are frequent across all the documents using TF-IDF (**term frequency-inverse document frequency** is a s**tatistical measure** used to evaluate the **importance of a word** to a **document in a collection or corpus**).\n",
    "\n",
    "---\n",
    "\n",
    "### Demonstration in Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 865,
     "status": "ok",
     "timestamp": 1607036393361,
     "user": {
      "displayName": "Kevin Dick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDswxDyFmSqBe3D1pfrZz6tWR7xhaF6MycS87d0Q=s64",
      "userId": "13066311860536673128"
     },
     "user_tz": 300
    },
    "id": "oxTz9BeY0JvQ",
    "outputId": "d070f4b2-d705-4d8a-bb75-c919d53dcaf2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kevindick/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/kevindick/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGjV8O2V0NRS"
   },
   "source": [
    " ### 1. Sentence Tokenization\n",
    " Split the text of string (from a document) into individuual sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 670,
     "status": "ok",
     "timestamp": 1607039386773,
     "user": {
      "displayName": "Kevin Dick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDswxDyFmSqBe3D1pfrZz6tWR7xhaF6MycS87d0Q=s64",
      "userId": "13066311860536673128"
     },
     "user_tz": 300
    },
    "id": "W_xAVCQhiqUi",
    "outputId": "8b6c3c31-205f-4597-fc65-5ed5c7242e2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: \"Backgammon is one of the oldest known board games.\"\n",
      "1: \"Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East.\"\n",
      "2: \"It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.\"\n"
     ]
    }
   ],
   "source": [
    "# For visual conveenience, this is represented as a string block, but it should\n",
    "# be conceptualized as a single long and continuous string.\n",
    "text = \"\"\"Backgammon is one of the oldest known board games. \n",
    "          Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East. \n",
    "          It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.\n",
    "       \"\"\"\n",
    "\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    print(f'{i}: \"{sentence}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWVt6R7z0ljY"
   },
   "source": [
    "### 2. Word Tokenization\n",
    "Split each sentence into individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 731,
     "status": "ok",
     "timestamp": 1607036399992,
     "user": {
      "displayName": "Kevin Dick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDswxDyFmSqBe3D1pfrZz6tWR7xhaF6MycS87d0Q=s64",
      "userId": "13066311860536673128"
     },
     "user_tz": 300
    },
    "id": "Kvwt81M30rJa",
    "outputId": "03d9548a-06fa-43e7-b8fd-85c5ed848a88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ['Backgammon', 'is', 'one', 'of', 'the', 'oldest', 'known', 'board', 'games', '.']\n",
      "1: ['Its', 'history', 'can', 'be', 'traced', 'back', 'nearly', '5,000', 'years', 'to', 'archeological', 'discoveries', 'in', 'the', 'Middle', 'East', '.']\n",
      "2: ['It', 'is', 'a', 'two', 'player', 'game', 'where', 'each', 'player', 'has', 'fifteen', 'checkers', 'which', 'move', 'between', 'twenty-four', 'points', 'according', 'to', 'the', 'roll', 'of', 'two', 'dice', '.']\n"
     ]
    }
   ],
   "source": [
    "for i, sentence in enumerate(sentences):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    print(f'{i}: {words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1_IDm5g1Xyx"
   },
   "source": [
    "### 3. Text Lemmatization & Stemming\n",
    "Reduce the semantic-space by reducing smilar words to a common semanttic token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 879,
     "status": "ok",
     "timestamp": 1607036890448,
     "user": {
      "displayName": "Kevin Dick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDswxDyFmSqBe3D1pfrZz6tWR7xhaF6MycS87d0Q=s64",
      "userId": "13066311860536673128"
     },
     "user_tz": 300
    },
    "id": "zoZzMIpj1v1z",
    "outputId": "2ff7bda3-5b1f-46fe-cd82-2223e6672e79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Word: seen\n",
      "----------\n",
      "Stemmer: seen\n",
      "Lemmatizer: see\n",
      "===============\n",
      "Input Word: drove\n",
      "----------\n",
      "Stemmer: drove\n",
      "Lemmatizer: drive\n",
      "===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kevindick/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def compare_stemmer_and_lemmatizer(stemmer, lemmatizer, word, pos):\n",
    "    \"\"\"\n",
    "    Print the results of stemming and lemmitization using the passed stemmer, \n",
    "    lemmatizer, word and pos (part of speech)\n",
    "    \"\"\"\n",
    "    print(f'Input Word: {word}\\n{\"-\" * 10}')\n",
    "    print(f'Stemmer: {stemmer.stem(word)}\\nLemmatizer: {lemmatizer.lemmatize(word, pos)}')\n",
    "    print('=' * 15)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "compare_stemmer_and_lemmatizer(stemmer, lemmatizer, word = \"seen\", pos = wordnet.VERB)\n",
    "compare_stemmer_and_lemmatizer(stemmer, lemmatizer, word = \"drove\", pos = wordnet.VERB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLaecDM63Wj4"
   },
   "source": [
    "### 4. Stop Words\n",
    "Remove high-frequency and \"noiisy\" words that don't add to the sentence meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 685,
     "status": "ok",
     "timestamp": 1607036970019,
     "user": {
      "displayName": "Kevin Dick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDswxDyFmSqBe3D1pfrZz6tWR7xhaF6MycS87d0Q=s64",
      "userId": "13066311860536673128"
     },
     "user_tz": 300
    },
    "id": "IBBmM7dn3hgM",
    "outputId": "48b9c1f7-3390-4679-bff1-4f0f98dc7828"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzcXRWvt3yoz"
   },
   "source": [
    "Let's remove the stop-words from a sentence:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1156,
     "status": "ok",
     "timestamp": 1607037072872,
     "user": {
      "displayName": "Kevin Dick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDswxDyFmSqBe3D1pfrZz6tWR7xhaF6MycS87d0Q=s64",
      "userId": "13066311860536673128"
     },
     "user_tz": 300
    },
    "id": "uZ7A9o6K3iFN",
    "outputId": "b6c38e44-7e41-46af-9972-6cd455115a0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Backgammon', 'one', 'oldest', 'known', 'board', 'games', '.']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "sentence = \"Backgammon is one of the oldest known board games.\"\n",
    "\n",
    "words = nltk.word_tokenize(sentence)\n",
    "without_stop_words = [word for word in words if not word in stop_words]\n",
    "print(without_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvuHOJWI5lTp"
   },
   "source": [
    "### 5. Regular Expression Filtering\n",
    "Regex are useful for filtering text based on specific patterns.\n",
    "\n",
    "Here, we remove all punctuation (remove anything that doesnt match a word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "nt0a5BIy37Ez"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The development of snowboarding was inspired by skateboarding  sledding  surfing and skiing \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "sentence = \"The development of snowboarding was inspired by skateboarding, sledding, surfing and skiing.\"\n",
    "pattern = r\"[^\\w]\" # Translates to: NOT MATCH WORD\n",
    "print(re.sub(pattern, \" \", sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_UdXTKB6XO6"
   },
   "source": [
    "* . - match any character except newline\n",
    "* \\w - match word\n",
    "* \\d - match digit\n",
    "* \\s - match whitespace\n",
    "* \\W - match not word\n",
    "* \\D - match not digit\n",
    "* \\S - match not whitespace\n",
    "* [abc] - match any of a, b, or c\n",
    "* [^abc] - not match a, b, or c\n",
    "* [a-g] - match a character between a & g\n",
    "\n",
    "### 6. Bag-of-Words\n",
    "\n",
    "This is a limited example of a Bag-of-Words application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "executionInfo": {
     "elapsed": 682,
     "status": "ok",
     "timestamp": 1607038378400,
     "user": {
      "displayName": "Kevin Dick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDswxDyFmSqBe3D1pfrZz6tWR7xhaF6MycS87d0Q=s64",
      "userId": "13066311860536673128"
     },
     "user_tz": 300
    },
    "id": "v3R71oCg6hOj",
    "outputId": "25ab1a96-d3ab-4aa2-f565-033f05fca156"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>awesome</th>\n",
       "      <th>funny</th>\n",
       "      <th>hate</th>\n",
       "      <th>it</th>\n",
       "      <th>like</th>\n",
       "      <th>love</th>\n",
       "      <th>movie</th>\n",
       "      <th>nice</th>\n",
       "      <th>one</th>\n",
       "      <th>this</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   awesome  funny  hate  it  like  love  movie  nice  one  this  was\n",
       "0        0      1     0   1     1     0      1     0    0     1    0\n",
       "1        0      0     1   0     0     0      1     0    0     1    0\n",
       "2        1      0     0   1     1     0      0     0    0     1    1\n",
       "3        0      0     0   1     0     1      0     1    1     0    0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the libraries we need\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Normally you would load this from a file.\n",
    "raw_text = \"\"\"I like this movie, it's funny. \n",
    "              I hate this movie.\n",
    "              This was awesome! I like it.\n",
    "              Nice one. I love it.\"\"\"\n",
    "\n",
    "# Step 1. Design the Vocabulary\n",
    "#   The default token pattern removes tokens of a single character. \n",
    "#   That's why we don't have the \"I\" and \"s\" tokens in the output\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Step 2. Create the Bag-of-Words Model\n",
    "bag_of_words = count_vectorizer.fit_transform(raw_text.splitlines())\n",
    "\n",
    "# Show the Bag-of-Words Model as a Pandas DataFrame\n",
    "#   NOTE: the sum of columns generates a \"concordance\" (SYSC 1005 Easter Egg!)\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "pd.DataFrame(bag_of_words.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltqt5qW17IHu"
   },
   "source": [
    "Considering a large amount of data in most big data applications, the length of the vector that represents a document might be **thousands or millions of elements.** Furthermore, each document may contain only a few of the known words in the vocabulary.\n",
    "\n",
    "Therefore the **vector representations will have a lot of zeros** and will therefore be **sparse vectors** that ttypically require **more memory and computational resources**.\n",
    "\n",
    "#### Vocabulary Simplification using *n*-grams: \n",
    "A more complex way to create a vocabulary is to use **grouped words**. This **changes the scope of the vocabulary** and allows the bag-of-words model to get more details about the document.\n",
    "\n",
    "### 7. TF-IDF: Term Frequency-Inverse Document Frequency\n",
    "The TF-IDF scoring value increases proportionally to the number of times a word appears in the document, but it is offset by the number of documents in the corpus that contain the word.\n",
    "\n",
    "![](https://miro.medium.com/max/700/1*V9ac4hLVyms79jl65Ym_Bw.png)\n",
    "\n",
    "**Term Frequency (TF)**: a scoring of the frequency of the word in the current document.\n",
    "\n",
    "![](https://miro.medium.com/max/463/1*V3qfsHl0t-bV5kA0mlnsjQ.png)\n",
    "\n",
    "**Inverse Document Frequency (IDF)**: a scoring of how rare the word is across documents.\n",
    "\n",
    "![](https://miro.medium.com/max/445/1*wvPGL02y36QL7-tdG1BT1A.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "executionInfo": {
     "elapsed": 696,
     "status": "ok",
     "timestamp": 1607039206346,
     "user": {
      "displayName": "Kevin Dick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDswxDyFmSqBe3D1pfrZz6tWR7xhaF6MycS87d0Q=s64",
      "userId": "13066311860536673128"
     },
     "user_tz": 300
    },
    "id": "fjzy7V3t-xve",
    "outputId": "e75db51d-b365-43a1-b17b-ec1ddc7ef221"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>awesome</th>\n",
       "      <th>funny</th>\n",
       "      <th>hate</th>\n",
       "      <th>it</th>\n",
       "      <th>like</th>\n",
       "      <th>love</th>\n",
       "      <th>movie</th>\n",
       "      <th>nice</th>\n",
       "      <th>one</th>\n",
       "      <th>this</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.571848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.365003</td>\n",
       "      <td>0.450852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.450852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.365003</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.702035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.553492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.448100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.539445</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.344321</td>\n",
       "      <td>0.425305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.344321</td>\n",
       "      <td>0.539445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.345783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    awesome     funny      hate        it      like      love     movie  \\\n",
       "0  0.000000  0.571848  0.000000  0.365003  0.450852  0.000000  0.450852   \n",
       "1  0.000000  0.000000  0.702035  0.000000  0.000000  0.000000  0.553492   \n",
       "2  0.539445  0.000000  0.000000  0.344321  0.425305  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.345783  0.000000  0.541736  0.000000   \n",
       "\n",
       "       nice       one      this       was  \n",
       "0  0.000000  0.000000  0.365003  0.000000  \n",
       "1  0.000000  0.000000  0.448100  0.000000  \n",
       "2  0.000000  0.000000  0.344321  0.539445  \n",
       "3  0.541736  0.541736  0.000000  0.000000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Normally you would load this from a file.\n",
    "raw_text = \"\"\"I like this movie, it's funny. \n",
    "              I hate this movie.\n",
    "              This was awesome! I like it.\n",
    "              Nice one. I love it.\"\"\"\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "values = tfidf_vectorizer.fit_transform(raw_text.splitlines())\n",
    "\n",
    "# Show the Model as a pandas DataFrame\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "pd.DataFrame(values.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cfP1GKWBFVK"
   },
   "source": [
    "---\n",
    "\n",
    "## PART II: Introduction to NLP Transformer-based Methods (NOTE: Time Consuming)\n",
    "The transformers library is an open-source, community-based repository to train, use and share models based on \n",
    "the Transformer architecture [(Vaswani & al., 2017)](https://arxiv.org/abs/1706.03762) such as Bert [(Devlin & al., 2018)](https://arxiv.org/abs/1810.04805),\n",
    "Roberta [(Liu & al., 2019)](https://arxiv.org/abs/1907.11692), GPT2 [(Radford & al., 2019)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf),\n",
    "XLNet [(Yang & al., 2019)](https://arxiv.org/abs/1906.08237), etc. \n",
    "\n",
    "Along with the models, the library contains multiple variations of each of them for a large variety of \n",
    "downstream-tasks like **Named Entity Recognition (NER)**, **Sentiment Analysis**, \n",
    "**Language Modeling**, **Question Answering** and so on.\n",
    "\n",
    "## The Recurrent Neural Networks that Preceded the Transformer\n",
    "\n",
    "In 2017, most Neural Network application to Natural Language Processing relied on the sequential processing of the input through [Recurrent Neural Network (RNN)](https://en.wikipedia.org/wiki/Recurrent_neural_network).\n",
    "\n",
    "![rnn](http://colah.github.io/posts/2015-09-NN-Types-FP/img/RNN-general.png)   \n",
    "\n",
    "RNNs performed well for a large variety of tasks involving sequential dependenciies over the input sequence, however, the sequentially-dependent process had issues modeling very long range dependencies and was not well suited for the kind of hardware we're currently leveraging (poor ability to parallelize computation). \n",
    "\n",
    "Most recently, the Attention mechanism was introduced as an improvement over \"raw\" RNNs by giving  a learned, weighted-importance to each element in the sequence, allowing the model to focus on \"important\" elements.\n",
    "\n",
    "![attention_rnn](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/08/Example-of-Attention.png)  \n",
    "\n",
    "## Then Came the Transformer  \n",
    "\n",
    "Then in 2017, [(Vaswani & al., 2017)](https://arxiv.org/abs/1706.03762)\n",
    "heralded the Transformer-era by demonstrating superiority over [Recurrent Neural Network (RNN)](https://en.wikipedia.org/wiki/Recurrent_neural_network)\n",
    "on translation tasks and Attention-based methods were quickly extended to almost all RNN-type tasks overcoming the State-of-the-Art at the time.\n",
    "\n",
    "One advantage of the Transformer architechtture over its RNN counterpart is its non-sequential attention model. Recall that RNNs have to iterate over each element of the input sequence one-by-one and carry an \"updatable-state\" between each hop. Conversely, Transformer models are able to look at every position in the sequence, at the same time, in one operation converting a formerly serial-type task into an embarassingly paarallel one.\n",
    "\n",
    "Read [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html#encoder-and-decoder-stacks) for a deep-dive into the Transformer architecture.\n",
    "\n",
    "![transformer-encoder-decoder](https://nlp.seas.harvard.edu/images/the-annotated-transformer_14_0.png)\n",
    "\n",
    "## Getting started with transformers\n",
    "\n",
    "For the rest of this notebook, we will use the [BERT (Devlin & al., 2018)](https://arxiv.org/abs/1810.04805) architecture, as it's the most simple and there are plenty of content about it over the internet (it will be easy to dig more over this architecture if you want to).\n",
    "\n",
    "The transformers library allows you to benefits from large, pretrained language models without requiring a huge and costly computational\n",
    "infrastructure. Most of the State-of-the-Art models are provided directly by their author and made available in the library \n",
    "in PyTorch and TensorFlow in a transparent and interchangeable way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QP6uSHWMQNjG"
   },
   "source": [
    "# How to train a new language model from scratch using Transformers and Tokenizers\n",
    "\n",
    "Over the past few months, we made several improvements to our [`transformers`](https://github.com/huggingface/transformers) and [`tokenizers`](https://github.com/huggingface/tokenizers) libraries, with the goal of making it easier than ever to **train a new language model from scratch**.\n",
    "\n",
    "In this post we’ll demo how to train a “small” model (84 M parameters = 6 layers, 768 hidden size, 12 attention heads) – that’s the same number of layers & heads as DistilBERT – on **Esperanto**. We’ll then fine-tune the model on a downstream task of part-of-speech tagging.\n",
    "\n",
    "\n",
    "## 1. Find a dataset\n",
    "\n",
    "First, let us find a corpus of text in Esperanto. Here we’ll use the Esperanto portion of the [OSCAR corpus](https://traces1.inria.fr/oscar/) from INRIA.\n",
    "OSCAR is a huge multilingual corpus obtained by language classification and filtering of [Common Crawl](https://commoncrawl.org/) dumps of the Web.\n",
    "\n",
    "<img src=\"https://huggingface.co/blog/assets/01_how-to-train/oscar.png\" style=\"margin: auto; display: block; width: 260px;\">\n",
    "\n",
    "The Esperanto portion of the dataset is only 299M, so we’ll concatenate with the Esperanto sub-corpus of the [Leipzig Corpora Collection](https://wortschatz.uni-leipzig.de/en/download), which is comprised of text from diverse sources like news, literature, and wikipedia.\n",
    "\n",
    "The final training corpus has a size of 3 GB, which is still small – for your model, you will get better results the more data you can get to pretrain on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2732,
     "status": "ok",
     "timestamp": 1607043473331,
     "user": {
      "displayName": "Kevin Dick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDswxDyFmSqBe3D1pfrZz6tWR7xhaF6MycS87d0Q=s64",
      "userId": "13066311860536673128"
     },
     "user_tz": 300
    },
    "id": "NLeODKeoQNDo",
    "outputId": "8cbfbe9a-dbdc-403d-fe0f-24b2bdc2a151"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-12-04 00:57:50--  https://cdn-datasets.huggingface.co/EsperBERTo/data/oscar.eo.txt\n",
      "Resolving cdn-datasets.huggingface.co (cdn-datasets.huggingface.co)... 54.230.86.67, 54.230.86.4, 54.230.86.53, ...\n",
      "Connecting to cdn-datasets.huggingface.co (cdn-datasets.huggingface.co)|54.230.86.67|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 312733741 (298M) [text/plain]\n",
      "Saving to: ‘oscar.eo.txt’\n",
      "\n",
      "oscar.eo.txt        100%[===================>] 298.25M   186MB/s    in 1.6s    \n",
      "\n",
      "2020-12-04 00:57:52 (186 MB/s) - ‘oscar.eo.txt’ saved [312733741/312733741]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in this notebook we'll only get one of the files (the Oscar one) for the sake of simplicity and performance\n",
    "!wget -c https://cdn-datasets.huggingface.co/EsperBERTo/data/oscar.eo.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vi8KNSNQZNT"
   },
   "source": [
    "## 2. Train a tokenizer\n",
    "\n",
    "We choose to train a byte-level Byte-pair encoding tokenizer (the same as GPT-2), with the same special tokens as RoBERTa. Let’s arbitrarily pick its size to be 52,000.\n",
    "\n",
    "We recommend training a byte-level BPE (rather than let’s say, a WordPiece tokenizer like BERT) because it will start building its vocabulary from an alphabet of single bytes, so all words will be decomposable into tokens (no more `<unk>` tokens!).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 732
    },
    "executionInfo": {
     "elapsed": 24387,
     "status": "ok",
     "timestamp": 1607043523575,
     "user": {
      "displayName": "Kevin Dick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDswxDyFmSqBe3D1pfrZz6tWR7xhaF6MycS87d0Q=s64",
      "userId": "13066311860536673128"
     },
     "user_tz": 300
    },
    "id": "1q_i98H1QZ8l",
    "outputId": "96537c69-d932-4e96-a85f-05b7bcce3b0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling tensorflow-2.3.1:\n",
      "  Successfully uninstalled tensorflow-2.3.1\n",
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-fwqmotb1\n",
      "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-fwqmotb1\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.0.dev0) (2019.12.20)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.0.dev0) (2.23.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.0.dev0) (4.41.1)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.0.dev0) (0.0.43)\n",
      "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.0.dev0) (0.9.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.0.dev0) (20.4)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.0.dev0) (0.8)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.0.dev0) (3.0.12)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.0.dev0) (1.18.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.1.0.dev0) (2020.11.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.1.0.dev0) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.1.0.dev0) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.1.0.dev0) (3.0.4)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.1.0.dev0) (0.17.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.1.0.dev0) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.1.0.dev0) (7.1.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==4.1.0.dev0) (2.4.7)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for transformers: filename=transformers-4.1.0.dev0-cp36-none-any.whl size=1375533 sha256=7417f3512f4b3e17a40ddae5582e31a6a10f7847d8d06ca19f3b092e16ec27fe\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-kid1jfl9/wheels/70/d3/52/b3fa4f8b8ef04167ac62e5bb2accb62ae764db2a378247490e\n",
      "Successfully built transformers\n",
      "Installing collected packages: transformers\n",
      "  Found existing installation: transformers 4.0.0\n",
      "    Uninstalling transformers-4.0.0:\n",
      "      Successfully uninstalled transformers-4.0.0\n",
      "Successfully installed transformers-4.1.0.dev0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "transformers"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizers                    0.9.4          \n",
      "transformers                  4.1.0.dev0     \n"
     ]
    }
   ],
   "source": [
    "# We won't need TensorFlow here\n",
    "!pip uninstall -y tensorflow\n",
    "# Install `transformers` from master\n",
    "!pip install git+https://github.com/huggingface/transformers\n",
    "!pip list | grep -E 'transformers|tokenizers'\n",
    "# transformers version at notebook update --- 2.11.0\n",
    "# tokenizers version at notebook update --- 0.8.0rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ChbPPd2GQjIT",
    "outputId": "6087e943-ee6f-44b0-c869-26708b540956"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18min 41s, sys: 4.64 s, total: 18min 46s\n",
      "Wall time: 9min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "paths = [str(x) for x in Path(\".\").glob(\"**/*.txt\")]\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 878,
     "status": "ok",
     "timestamp": 1607044201257,
     "user": {
      "displayName": "Kevin Dick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDswxDyFmSqBe3D1pfrZz6tWR7xhaF6MycS87d0Q=s64",
      "userId": "13066311860536673128"
     },
     "user_tz": 300
    },
    "id": "V1SQChFnTG6F",
    "outputId": "928fc6e9-6f62-4c0a-a6aa-934bdf507f54"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EsperBERTo/vocab.json', 'EsperBERTo/merges.txt']"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!mkdir EsperBERTo\n",
    "tokenizer.save_model(\"EsperBERTo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwaKLs1rTOny"
   },
   "source": [
    "What is great is that our tokenizer is optimized for Esperanto. Compared to a generic tokenizer trained for English, more native words are represented by a single, unsplit token. Diacritics, i.e. accented characters used in Esperanto – `ĉ`, `ĝ`, `ĥ`, `ĵ`, `ŝ`, and `ŭ` – are encoded natively. We also represent sequences in a more efficient manner. Here on this corpus, the average length of encoded sequences is ~30% smaller as when using the pretrained GPT-2 tokenizer.\n",
    "\n",
    "Here’s  how you can use it in `tokenizers`, including handling the RoBERTa special tokens – of course, you’ll also be able to use it directly from `transformers`.\n",
    "\n",
    "We now have both a `vocab.json`, which is a list of the most frequent tokens ranked by frequency, and a `merges.txt` list of merges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 784,
     "status": "ok",
     "timestamp": 1607044297864,
     "user": {
      "displayName": "Kevin Dick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDswxDyFmSqBe3D1pfrZz6tWR7xhaF6MycS87d0Q=s64",
      "userId": "13066311860536673128"
     },
     "user_tz": 300
    },
    "id": "JzTMGOyCTejv"
   },
   "outputs": [],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"./EsperBERTo/vocab.json\",\n",
    "    \"./EsperBERTo/merges.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 805,
     "status": "ok",
     "timestamp": 1607044310768,
     "user": {
      "displayName": "Kevin Dick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDswxDyFmSqBe3D1pfrZz6tWR7xhaF6MycS87d0Q=s64",
      "userId": "13066311860536673128"
     },
     "user_tz": 300
    },
    "id": "PP3uwf9SThy5"
   },
   "outputs": [],
   "source": [
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 665,
     "status": "ok",
     "timestamp": 1607044922176,
     "user": {
      "displayName": "Kevin Dick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDswxDyFmSqBe3D1pfrZz6tWR7xhaF6MycS87d0Q=s64",
      "userId": "13066311860536673128"
     },
     "user_tz": 300
    },
    "id": "ItZJfD6nTqdO",
    "outputId": "8991f193-0c41-4596-e78d-09c22d1486e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'Mi', 'Ġestas', 'ĠKe', 'vin', '.', '</s>']"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Mi estas Kevin.\").tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HJ2_xL6V_r_"
   },
   "source": [
    "**bold text**## 3. Train a language model from scratch\n",
    "\n",
    "**Update:** This section follows along the [`run_language_modeling.py`](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_language_modeling.py) script, using our new [`Trainer`](https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py) directly. Feel free to pick the approach you like best.\n",
    "\n",
    "> We’ll train a RoBERTa-like model, which is a BERT-like with a couple of changes (check the [documentation](https://huggingface.co/transformers/model_doc/roberta.html) for more details).\n",
    "\n",
    "As the model is BERT-like, we’ll train it on a task of *Masked language modeling*, i.e. the predict how to fill arbitrary tokens that we randomly mask in the dataset. This is taken care of by the example script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 819,
     "status": "ok",
     "timestamp": 1607044967266,
     "user": {
      "displayName": "Kevin Dick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDswxDyFmSqBe3D1pfrZz6tWR7xhaF6MycS87d0Q=s64",
      "userId": "13066311860536673128"
     },
     "user_tz": 300
    },
    "id": "YoiRlv-UWARS",
    "outputId": "0e3407a2-efe8-4eaa-cf45-d0d60426b196"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check that we have a GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EUUgUOfbWHA2"
   },
   "outputs": [],
   "source": [
    "# Check that PyTorch sees it\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJWdceEgWJUf"
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=52_000,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5RXlwtvjWL4d"
   },
   "outputs": [],
   "source": [
    "# Now let's re-create our tokenizer in transformers\n",
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"./EsperBERTo\", max_len=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7dU72l1WSEx"
   },
   "source": [
    "Finally let's initialize our model.\n",
    "\n",
    "**Important:**\n",
    "\n",
    "As we are training from scratch, we only initialize from a config, not from an existing pretrained model or checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sEC1gmylWUES"
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "model.num_parameters()\n",
    "# => 84 million parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlOXDEE8WY0t"
   },
   "source": [
    "### Now let's build our training Dataset\n",
    "\n",
    "We'll build our dataset by applying our tokenizer to our text file.\n",
    "\n",
    "Here, as we only have one text file, we don't even need to customize our `Dataset`. We'll just use the `LineByLineDataset` out-of-the-box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TxMVbP0FWa-x"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from transformers import LineByLineTextDataset\n",
    "\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"./oscar.eo.txt\",\n",
    "    block_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Rco5XoOWc_e"
   },
   "source": [
    "Like in the [`run_language_modeling.py`](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_language_modeling.py) script, we need to define a data_collator.\n",
    "\n",
    "This is just a small helper that will help us batch different samples of the dataset together into an object that PyTorch knows how to perform backprop on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PB9DKc3uWe-1"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icYoXr7SWiVy"
   },
   "source": [
    "### Finally, we are all set to initialize our Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8mLxWFrZWkfL"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./EsperBERTo\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_gpu_train_batch_size=64,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    "    prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qImPT0TLWpDk"
   },
   "source": [
    "#### Start Training (~1hr+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ank2Pww5WoN4"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iec9qQ4wWwBH"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"./EsperBERTo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUjPux70W0G1"
   },
   "source": [
    "#### Check that the Language Model Actually Learned\n",
    "\n",
    "Aside from looking at the training and eval losses going down, the easiest way to check whether our language model is learning anything interesting is via the `FillMaskPipeline`.\n",
    "\n",
    "Pipelines are simple wrappers around tokenizers and models, and the 'fill-mask' one will let you input a sequence containing a masked token (here, `<mask>`) and return a list of the most probable filled sequences, with their probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5a87vBe-W9Tr"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"./EsperBERTo\",\n",
    "    tokenizer=\"./EsperBERTo\"\n",
    ")\n",
    "\n",
    "# The sun <mask>.\n",
    "# =>\n",
    "\n",
    "fill_mask(\"La suno <mask>.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNVgnw8YXF8a"
   },
   "source": [
    "Ok, simple syntax/grammar works. Let’s try a slightly more interesting prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O90uVhXgXE-O"
   },
   "outputs": [],
   "source": [
    "fill_mask(\"Jen la komenco de bela <mask>.\")\n",
    "\n",
    "# This is the beginning of a beautiful <mask>.\n",
    "# =>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZVBSx1RXLYd"
   },
   "source": [
    "### Share your Model with the Community\n",
    "\n",
    "Finally, when you have a nice model, please think about sharing it with the community:\n",
    "\n",
    "- upload your model using the CLI: `transformers-cli upload`\n",
    "- write a README.md model card and add it to the repository under `model_cards/`. Your model card should ideally include:\n",
    "    - a model description,\n",
    "    - training params (dataset, preprocessing, hyperparameters), \n",
    "    - evaluation results,\n",
    "    - intended uses & limitations\n",
    "    - whatever else is helpful! 🤓\n",
    "\n",
    "### **TADA!**\n",
    "\n",
    "➡️ Your model has a page on http://huggingface.co/models and everyone can load it using `AutoModel.from_pretrained(\"username/model_name\")`.\n",
    "\n",
    "[![tb](https://huggingface.co/blog/assets/01_how-to-train/model_page.png)](https://huggingface.co/julien-c/EsperBERTo-small)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Part III: Topic Modelling using Laten Dirichelet Allocation\n",
    "\n",
    "In this final part, we will demonstrate how to perform Topic Modelling on a corpus of data.\n",
    "As a reminder, a \"corpus\" means a collection of text documents and can be of arbitrary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/Users/kevindick/Library/Caches/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: gensim in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (3.8.3)\n",
      "Requirement already satisfied: spacy in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (2.3.4)\n",
      "Collecting pyLDAvis\n",
      "  Downloading pyLDAvis-2.1.2.tar.gz (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5.0 in /Users/kevindick/Library/Python/3.8/lib/python/site-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from gensim) (1.5.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from gensim) (4.0.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from gensim) (1.19.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy) (4.45.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/kevindick/Library/Python/3.8/lib/python/site-packages (from spacy) (2.24.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy) (3.0.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy) (0.8.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy) (2.0.4)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy) (7.4.3)\n",
      "Requirement already satisfied: setuptools in /Users/kevindick/Library/Python/3.8/lib/python/site-packages (from spacy) (50.3.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0; python_version >= \"3.6\" in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy) (0.7.3)\n",
      "Requirement already satisfied: wheel>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pyLDAvis) (0.35.1)\n",
      "Requirement already satisfied: pandas>=0.17.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pyLDAvis) (0.24.2)\n",
      "Requirement already satisfied: joblib>=0.8.4 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pyLDAvis) (0.16.0)\n",
      "Requirement already satisfied: jinja2>=2.7.2 in /Users/kevindick/Library/Python/3.8/lib/python/site-packages (from pyLDAvis) (2.11.2)\n",
      "Collecting numexpr\n",
      "  Downloading numexpr-2.7.1-cp38-cp38-macosx_10_9_x86_64.whl (101 kB)\n",
      "\u001b[K     |████████████████████████████████| 101 kB 7.5 MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting pytest\n",
      "  Downloading pytest-6.1.2-py3-none-any.whl (272 kB)\n",
      "\u001b[K     |████████████████████████████████| 272 kB 5.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: future in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pyLDAvis) (0.18.2)\n",
      "Collecting funcy\n",
      "  Downloading funcy-1.15-py2.py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/kevindick/Library/Python/3.8/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.10)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/kevindick/Library/Python/3.8/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/kevindick/Library/Python/3.8/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kevindick/Library/Python/3.8/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /Users/kevindick/Library/Python/3.8/lib/python/site-packages (from pandas>=0.17.0->pyLDAvis) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2011k in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pandas>=0.17.0->pyLDAvis) (2019.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/kevindick/Library/Python/3.8/lib/python/site-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n",
      "Collecting py>=1.8.2\n",
      "  Downloading py-1.9.0-py2.py3-none-any.whl (99 kB)\n",
      "\u001b[K     |████████████████████████████████| 99 kB 4.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.4.0 in /Users/kevindick/Library/Python/3.8/lib/python/site-packages (from pytest->pyLDAvis) (20.2.0)\n",
      "Collecting iniconfig\n",
      "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
      "Requirement already satisfied: toml in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pytest->pyLDAvis) (0.10.1)\n",
      "Requirement already satisfied: packaging in /Users/kevindick/Library/Python/3.8/lib/python/site-packages (from pytest->pyLDAvis) (20.4)\n",
      "Collecting pluggy<1.0,>=0.12\n",
      "  Downloading pluggy-0.13.1-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/kevindick/Library/Python/3.8/lib/python/site-packages (from packaging->pytest->pyLDAvis) (2.4.7)\n",
      "Building wheels for collected packages: pyLDAvis\n",
      "  Building wheel for pyLDAvis (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyLDAvis: filename=pyLDAvis-2.1.2-py2.py3-none-any.whl size=97712 sha256=18703478a504aee49913ab90ca0b0714ec68043b223fda36093e54a3a1b0fd0c\n",
      "  Stored in directory: /private/tmp/pip-ephem-wheel-cache-ncu3zqp7/wheels/31/8c/a0/24a443892f2134e691d59c8c6c35e19821e02f85e49871f8fd\n",
      "Successfully built pyLDAvis\n",
      "Installing collected packages: numexpr, py, iniconfig, pluggy, pytest, funcy, pyLDAvis\n",
      "Successfully installed funcy-1.15 iniconfig-1.1.1 numexpr-2.7.1 pluggy-0.13.1 py-1.9.0 pyLDAvis-2.1.2 pytest-6.1.2\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim spacy pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.8.2 (v3.8.2:7b3ab5921f, Feb 24 2020, 17:52:18) \n",
      "[Clang 6.0 (clang-600.0.57)]\n"
     ]
    }
   ],
   "source": [
    "import re, gensim, os, sys, spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim import models\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print('Python Version: %s' % (sys.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevindick/Library/Python/3.8/lib/python/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'documents.dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-00507e520ee6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'documents.dict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMmCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'documents.mm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlda_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLdaModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lda_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mldamallet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLdaMallet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ldamallet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimal_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLdaMallet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'optimal_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m     \"\"\"\n\u001b[0;32m-> 1395\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1396\u001b[0m         \u001b[0;31m# Because of loading from S3 load can't be used (missing readline in smart_open)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mtransport_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     fobj = _shortcut_open(\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, ignore_ext, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'documents.dict'"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary.load('documents.dict')\n",
    "corpus = gensim.corpora.MmCorpus('documents.mm')\n",
    "lda_model = models.LdaModel.load('lda_model')\n",
    "ldamallet = models.wrappers.LdaMallet.load('ldamallet')\n",
    "optimal_model = models.wrappers.LdaMallet.load('optimal_model')\n",
    "\n",
    "print(dictionary)\n",
    "print(corpus)\n",
    "print(lda_model)\n",
    "print(ldamallet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#with open('documents', 'wb') as f: #save\n",
    "#    pickle.dump(mylist, f)\n",
    "\n",
    "with open('documents', 'rb') as f: #load\n",
    "    documents = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Clean-up using gensim’s simple_preprocess()\n",
    "The sentences look better now, but you want to tokenize each sentence into a list of words, removing punctuations and unnecessary characters altogether.\n",
    "\n",
    "Gensim’s `simple_preprocess()` is great for this. Additionally I have set deacc=True to remove the punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "Lemmatization is a process where we convert words to its root word.\n",
    "\n",
    "For example: ‘Studying’ becomes ‘Study’, ‘Meeting becomes ‘Meet’, ‘Better’ and ‘Best’ becomes ‘Good’.\n",
    "\n",
    "The advantage of this is, we get to reduce the total number of unique words in the dictionary. As a result, the number of columns in the document-word matrix (created by CountVectorizer in the next step) will be denser with lesser columns.\n",
    "\n",
    "You can expect better topics to be generated in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
    "    return texts_out\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# Run in terminal: python3 -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only Noun, Adj, Verb, Adverb\n",
    "data_lemmatized = lemmatization(documents, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Document-Word matrix\n",
    "The LDA topic model algorithm requires a document word matrix as the main input.\n",
    "\n",
    "You can create one using CountVectorizer. In the below code, I have configured the `CountVectorizer` to consider words that has occurred at least 10 times (min_df), remove built-in english stopwords, convert all words to lowercase, and a word can contain numbers and alphabets of at least length 3 in order to be qualified as a word.\n",
    "\n",
    "So, to create the doc-word matrix, you need to first initialise the CountVectorizer class with the required configuration and then apply fit_transform to actually create the matrix.\n",
    "\n",
    "Since most cells contain zeros, the result will be in the form of a sparse matrix to save memory.\n",
    "\n",
    "If you want to materialize it in a 2D array format, call the 1todense()1 method of the sparse matrix like its done in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=4,                        # minimum reqd occurences of a word \n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                             # max_features=50000,             # max number of uniq words\n",
    "                            )\n",
    "\n",
    "data_vectorized = vectorizer.fit_transform(data_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the Sparsicity\n",
    "Sparsicity is nothing but the percentage of non-zero datapoints in the document-word matrix, that is data_vectorized.\n",
    "\n",
    "Since most cells in this matrix will be zero, I am interested in knowing what percentage of cells contain non-zero values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Materialize the sparse data\n",
    "data_dense = data_vectorized.todense()\n",
    "\n",
    "# Compute Sparsicity = Percentage of Non-Zero cells\n",
    "print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build LDA model with sklearn\n",
    "Everything is ready to build a Latent Dirichlet Allocation (LDA) model. Let’s initialise one and call `fit_transform()` to build the LDA model.\n",
    "\n",
    "For this example, I have set the n_topics as 20 based on prior knowledge about the dataset. Later we will find the optimal number using grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA Model\n",
    "lda_model = LatentDirichletAllocation(n_components=20,               # Number of topics\n",
    "                                      max_iter=10,               # Max learning iterations\n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,          # Random state\n",
    "                                      batch_size=128,            # n docs in each learning iter\n",
    "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                      n_jobs = -1,               # Use all available CPUs\n",
    "                                     )\n",
    "lda_output = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "print(lda_model)  # Model attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnose model performance with perplexity and log-likelihood\n",
    "\n",
    "A model with higher log-likelihood and lower perplexity (exp(-1. * log-likelihood per word)) is considered to be good. Let’s check for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Likelyhood: Higher the better\n",
    "print(\"Log Likelihood: \", lda_model.score(data_vectorized))\n",
    "\n",
    "# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "print(\"Perplexity: \", lda_model.perplexity(data_vectorized))\n",
    "\n",
    "# See model parameters\n",
    "print(lda_model.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to GridSearch the best LDA model?\n",
    "The most important tuning parameter for LDA models is `n_components` (number of topics). In addition, I am going to search `learning_decay` (which controls the learning rate) as well.\n",
    "\n",
    "Besides these, other possible search params could be `learning_offset` (downweigh early iterations. Should be `> 1) and max_iter`. These could be worth experimenting if you have enough computing resources.\n",
    "\n",
    "Be warned, the grid search constructs multiple LDA models for all possible combinations of param values in the param_grid dict. So, this process can consume a lot of time and resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Search Param\n",
    "search_params = {'n_components': [10, 15, 20, 25, 30], 'learning_decay': [.5, .7, .9]}\n",
    "\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation()\n",
    "\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "\n",
    "# Do the Grid Search\n",
    "model.fit(data_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to see the best topic model and its parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Model\n",
    "best_lda_model = model.best_estimator_\n",
    "\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Log Likelyhoods from Grid Search Output\n",
    "log_likelyhoods_5 = []\n",
    "log_likelyhoods_7 = []\n",
    "log_likelyhoods_9 = []\n",
    "\n",
    "for i in range(len(model.cv_results_['params'])):\n",
    "    if model.cv_results_['params'][i]['learning_decay'] == 0.5:\n",
    "       log_likelyhoods_5.append(round(model.cv_results_['mean_test_score'][i]))\n",
    "    elif model.cv_results_['params'][i]['learning_decay'] == 0.7:\n",
    "       log_likelyhoods_7.append(round(model.cv_results_['mean_test_score'][i]))\n",
    "    elif model.cv_results_['params'][i]['learning_decay'] == 0.9:\n",
    "       log_likelyhoods_9.append(round(model.cv_results_['mean_test_score'][i]))\n",
    "\n",
    "# Show graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(n_topics, log_likelyhoods_5, label='0.5')\n",
    "plt.plot(n_topics, log_likelyhoods_7, label='0.7')\n",
    "plt.plot(n_topics, log_likelyhoods_9, label='0.9')\n",
    "plt.title(\"Choosing Optimal LDA Model\")\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Log Likelyhood Scores\")\n",
    "plt.legend(title='Learning decay', loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to see the dominant topic in each document?\n",
    "To classify a document as belonging to a particular topic, a logical approach is to see which topic has the highest contribution to that document and assign it.\n",
    "\n",
    "In the table below, I’ve greened out all major topics in a document and assigned the most dominant topic in its own column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Document - Topic Matrix\n",
    "lda_output = best_lda_model.transform(data_vectorized)\n",
    "\n",
    "# column names\n",
    "topicnames = [\"Topic\" + str(i) for i in range(best_lda_model.n_components)]\n",
    "\n",
    "# index names\n",
    "docnames = [\"Doc\" + str(i) for i in range(len(documents))]\n",
    "\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['dominant_topic'] = dominant_topic\n",
    "\n",
    "# Styling\n",
    "def color_green(val):\n",
    "    color = 'green' if val > .1 else 'black'\n",
    "    return 'color: {col}'.format(col=color)\n",
    "\n",
    "def make_bold(val):\n",
    "    weight = 700 if val > .1 else 400\n",
    "    return 'font-weight: {weight}'.format(weight=weight)\n",
    "\n",
    "# Apply Style\n",
    "df_document_topics = df_document_topic.head(15).style.applymap(color_green).applymap(make_bold)\n",
    "df_document_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review topics distribution across documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_distribution = df_document_topic['dominant_topic'].value_counts().reset_index(name=\"Num Documents\")\n",
    "df_topic_distribution.columns = ['Topic Num', 'Num Documents']\n",
    "df_topic_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to visualize the LDA model with pyLDAvis?\n",
    "The pyLDAvis offers the best visualization to view the topics-keywords distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(best_lda_model, data_vectorized, vectorizer, mds='tsne')\n",
    "panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to see the Topic’s keywords?\n",
    "The weights of each keyword in each topic is contained in `lda_model.components_` as a 2d array. The names of the keywords itself can be obtained from vectorizer object using `get_feature_names()`.\n",
    "\n",
    "Let’s use this info to construct a weight matrix for all keywords in each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic-Keyword Matrix\n",
    "df_topic_keywords = pd.DataFrame(best_lda_model.components_)\n",
    "\n",
    "# Assign Column and Index\n",
    "df_topic_keywords.columns = vectorizer.get_feature_names()\n",
    "df_topic_keywords.index = topicnames\n",
    "\n",
    "# View\n",
    "df_topic_keywords.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the top 15 keywords each topic\n",
    "From the above output, I want to see the top 15 keywords that are representative of the topic.\n",
    "\n",
    "The `show_topics()` defined below creates that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top n keywords for each topic\n",
    "def show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=20):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n",
    "\n",
    "topic_keywords = show_topics(vectorizer=vectorizer, lda_model=best_lda_model, n_words=15)        \n",
    "\n",
    "# Topic - Keywords Dataframe\n",
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "df_topic_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to predict the topics for a new piece of text?\n",
    "Assuming that you have already built the topic model, you need to take the text through the same routine of transformations and before predicting the topic.\n",
    "\n",
    "For our case, the order of transformations is:\n",
    "\n",
    "`sent_to_words() –> lemmatization() –> vectorizer.transform() –> best_lda_model.transform()`\n",
    "\n",
    "You need to apply these transformations in the same order. So to simplify it, let’s combine these steps into a `predict_topic()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to predict topic for a given text document.\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "def predict_topic(text, nlp=nlp):\n",
    "    global sent_to_words\n",
    "    global lemmatization\n",
    "\n",
    "    # Step 1: Clean with simple_preprocess\n",
    "    mytext_2 = list(sent_to_words(text))\n",
    "\n",
    "    # Step 2: Lemmatize\n",
    "    mytext_3 = lemmatization(mytext_2, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "    # Step 3: Vectorize transform\n",
    "    mytext_4 = vectorizer.transform(mytext_3)\n",
    "\n",
    "    # Step 4: LDA Transform\n",
    "    topic_probability_scores = best_lda_model.transform(mytext_4)\n",
    "    topic = df_topic_keywords.iloc[np.argmax(topic_probability_scores), :].values.tolist()\n",
    "    return topic, topic_probability_scores\n",
    "\n",
    "# Predict the topic\n",
    "mytext = [\"Some text about christianity and bible\"]\n",
    "topic, prob_scores = predict_topic(text = mytext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to cluster documents that share similar topics and plot?\n",
    "You can use k-means clustering on the document-topic probabilioty matrix, which is nothing but lda_output object. Since out best model has 4 clusters, I’ve set `n_clusters=4` `in KMeans()`.\n",
    "\n",
    "Alternately, you could avoid k-means and instead, assign the cluster as the topic column number with the highest probability score.\n",
    "\n",
    "We now have the cluster number. But we also need the X and Y columns to draw the plot.\n",
    "\n",
    "For the X and Y, you can use SVD on the `lda_output` object with `n_components` as 2. SVD ensures that these two columns captures the maximum possible amount of information from `lda_output` in the first 2 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the k-means clusters\n",
    "from sklearn.cluster import KMeans\n",
    "clusters = KMeans(n_clusters=4, random_state=100).fit_predict(lda_output)\n",
    "\n",
    "# Build the Singular Value Decomposition(SVD) model\n",
    "svd_model = TruncatedSVD(n_components=2)  # 2 components\n",
    "lda_output_svd = svd_model.fit_transform(lda_output)\n",
    "\n",
    "# X and Y axes of the plot using SVD decomposition\n",
    "x = lda_output_svd[:, 0]\n",
    "y = lda_output_svd[:, 1]\n",
    "\n",
    "# Weights for the 15 columns of lda_output, for each component\n",
    "print(\"Component's weights: \\n\", np.round(svd_model.components_, 2))\n",
    "\n",
    "# Percentage of total information in 'lda_output' explained by the two components\n",
    "print(\"Perc of Variance Explained: \\n\", np.round(svd_model.explained_variance_ratio_, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the X, Y and the cluster number for each document.\n",
    "\n",
    "Let’s plot the document along the two SVD decomposed components. The color of points represents the cluster number (in this case) or topic number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.scatter(x, y, c=clusters)\n",
    "plt.xlabel('Component 2')\n",
    "plt.xlabel('Component 1')\n",
    "plt.title(\"Segregation of Topic Clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to get similar documents for any given piece of text?\n",
    "Once you know the probaility of topics for a given document (using `predict_topic()`), compute the euclidean distance with the probability scores of all other documents.\n",
    "\n",
    "The most similar documents are the ones with the smallest distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "def similar_documents(text, doc_topic_probs, documents = documents, nlp=nlp, top_n=5, verbose=False):\n",
    "    topic, x  = predict_topic(text)\n",
    "    dists = euclidean_distances(x.reshape(1, -1), doc_topic_probs)[0]\n",
    "    doc_ids = np.argsort(dists)[:top_n]\n",
    "    if verbose:        \n",
    "        print(\"Topic KeyWords: \", topic)\n",
    "        print(\"Topic Prob Scores of text: \", np.round(x, 1))\n",
    "        print(\"Most Similar Doc's Probs:  \", np.round(doc_topic_probs[doc_ids], 1))\n",
    "    return doc_ids, np.take(documents, doc_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get similar documents\n",
    "mytext = [\"Some text about christianity and bible\"]\n",
    "doc_ids, docs = similar_documents(text=mytext, doc_topic_probs=lda_output, documents = documents, top_n=1, verbose=True)\n",
    "print('\\n', docs[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Takeaway Messages:\n",
    "\n",
    "- NLP is a **subfield of machine learning** concerned with the application of **learning algorthms to text and speech**.\n",
    "- NLP-based methods are typically applicable to all sequential-type information (*e.g.* DNA sequences, audio signals, time-series signals, *etc.*) however, they are predominantly used in human language applications.\n",
    "- For example, we can use NLP to create systems including:\n",
    "    1. **speech recognition** (*e.g.* real-time captioning)\n",
    "    2. **document summarization**\n",
    "    3. **machine translation**\n",
    "    4. **spam detection**\n",
    "    5. **named entity recognition**\n",
    "    6. **question answering**\n",
    "    7. **autocomplete** (*i.e.* predictive typing)\n",
    "- **Sentence Tokenization:** Sentence tokenization (also called **sentence segmentation**) is the problem of *dividing a string of written language into its component sentences*.\n",
    "- **Word Tokenization**: Word tokenization (also called **word segmentation**) is the problem of *dividing a string of written language into its component words*.\n",
    "- **Text Lemmatization & Stemming**: The goal of both stemming (crude) and lemmatization (refiined) is to *reduce inflectional forms* and sometimes derivationally related forms of a word to a common base form. For example, \"drive\" & \"drives\" & \"driving\" all have the same semantic meaning and should be combined.\n",
    "- **Stop Words**: Stop words usually refer to the **most common words** such as “and”, “the”, “a” in a language and when applying machine learning to text, **these words can add a lot of noise** so we remove them.\n",
    "- **Regex**: A regular expression, regex, or regexp is a sequence of characters that define *a search pattern to apply additional filtering* to our text. For example, we can remove all the non-words characters. In many cases, we don’t need the punctuation marks and it’s easy to remove them with regex.\n",
    "- **Bag-of-Words**: Machine learning algorithms *cannot work with raw text directly*, we need to convert the text into vectors of numbers (i.e. feature extraction) and the *bag-of-words mode*l is a popular and simple feature extraction technique that *counts the occurrence of each word within a document*.\n",
    "- **TF-IDF**: One problem with scoring word frequency is that *the most frequent words in the document start to have the highest scores* (frequent words may not havee much “informational gain”) to the model so we penalize words that are frequent across all the documents using TF-IDF (**term frequency-inverse document frequency** is a s**tatistical measure** used to evaluate the **importance of a word** to a **document in a collection or corpus**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPhqHZA+OO1DvrxmKAHr4qM",
   "collapsed_sections": [],
   "name": "Tutorial-11_NLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
